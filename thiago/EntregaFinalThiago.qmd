
Este código en R se encarga de realizar una búsqueda de noticias en una fuente específica (en este caso, "pagina12.com.ar") en un rango de fechas definido, recuperar los resultados de la búsqueda desde la API de GDELT, almacenar los datos en un DataFrame, realizar una deduplicación de los resultados y exportarlos a un archivo CSV. Aquí está el desglose del código paso a paso:

Vamos a usar la fuente de Pagina12 para esta entrega.

```{r}
#########################
if (!require("tidyverse")) install.packages("tidyverse")
if (!require("readr")) install.packages("readr")
if (!require("dplyr")) install.packages("dplyr")
if (!require("tidytext")) install.packages("tidytext")
library(tidyverse)
library(readr)
library(dplyr)
library(tidytext)
library(stringr) # Part of the tidyverse package

#########################
query <- " " #Enter search term(s)
startdate <- "20230201" #Enter preferred start date
enddate <- "20230815" #Enter preferred end date
sources <- c("pagina12.com.ar") #Enter sources to search

#########################
#Generating a sequence of dates
startdate2 <- as.Date(startdate,"%Y%m%d")
enddate2 <- as.Date(enddate,"%Y%m%d")
dates <- seq(as.Date(startdate2), as.Date(enddate2), "days")
dates <- format(dates, "%Y%m%d")
#Estimating run time for query
Minutes <- round((length(sources)*(length(dates)*2.5/60)), digits = 1)
Hours <- round((length(sources)*(length(dates)*2.5/3600)), digits = 1)
print(paste0("In minutes: ",Minutes,". In hours: ",Hours,"."))

#########################
#Creating the dataframe
Headlines = data.frame(
  Source = character(),
  URL = character(),
  MobileURL = character(),
  Date = character(),
  Title = character(),
  stringsAsFactors = FALSE)
# For loops
for (thissource in sources)
{  
  for (thisdate in dates)
  {
    URL_p1 <- "https://api.gdeltproject.org/api/v2/doc/doc?query="
    URL_p2 <- " domainis:"
    URL_p3 <- "&mode=artlist&maxrecords=250&sort=datedesc&startdatetime="
    URL_p4 <- "000000&enddatetime="
    URL_p5 <- "235959&format=CSV"
    URL_raw <- paste0(URL_p1,
                      query,
                      URL_p2,
                      thissource,
                      URL_p3,
                      thisdate,
                      URL_p4,
                      thisdate,
                      URL_p5)
    URL_encoded <- URLencode(URL_raw)
    print("Getting data for")
    print(paste0(thisdate," ",thissource))
    print(URL_encoded)
    Thisfetch <- read_csv(URL_encoded, show_col_types = FALSE)
    Thisfetch$Source <- thissource
    print(paste0(nrow(Thisfetch)," rows"))
    Headlines <- rbind(Headlines,Thisfetch)
    #Sys.sleep(2)
  }
}
#Deduplicate, check, and export data
Headlines <- Headlines[!duplicated(Headlines$URL),]
CountsBySource <- Headlines %>% 
  group_by(Source) %>% 
  summarize(HeadlineCount = n())
View(CountsBySource)
filename <- paste0(query,"-",startdate,"-",enddate,".csv")
write_excel_csv(Headlines,filename)
#Cleanup
rm(Thisfetch,
   dates,
   enddate,
   enddate2,
   filename,
   query,
   sources,
   startdate,
   startdate2,
   thisdate,
   thissource,
   URL_encoded,
   URL_p1,
   URL_p2,
   URL_p3,
   URL_p4,
   URL_p5,
   URL_raw)
#View raw data frame
View(Headlines)

#########################
#Headline word counts
WordCounts <- Headlines %>% 
  unnest_tokens(word,Title) %>% 
  count(word, sort = TRUE)
# Deleting standard stop words
data("stop_words")
WordCounts <- WordCounts %>%
  anti_join(stop_words)
# Deleting custom stop words
my_stopwords <- tibble(word = c("and",
                                "the",
                                "etc."))
WordCounts <- WordCounts %>% 
  anti_join(my_stopwords)
rm(stop_words,
   my_stopwords)
#Viewing word counts
View(WordCounts)
```

Siguiente paso, una vez que tenemos las noticias recopiladas, es extraer el contenido de ellas.

```{r}
library(readr)
source(here::here("R", "scrape-news.R"))

mydata <- read_csv(here::here("data", "pagina12-20230201-20230815.csv"))


my_links <- mydata$URL
purrr::map(my_links, ~{
  scrape_news(.x, title_tag = "h1", content_tag = ".article-main-content.article-text")
}) %>% 
  bind_rows() -> my_data

output_filename <- here::here("data", "scraping-pagina12-2023-10-09.rds")

readr::write_rds(my_data, output_filename)
```

Luego, contamos qué palabras se repiten en esos contenidos.

```{r}
# Instala y carga la biblioteca necesaria (si no lo has hecho antes)
##install.packages("tm")
##install.packages("matrixStats")  # Agregar esta línea para corregir el error
library(matrixStats)  # Agregar esta línea para cargar matrixStats
library(tm)
library(tidyverse)
library(tidytext)
library(ggwordcloud)
library(stopwords)
library(topicmodels)
library(ldatuning)
library(syuzhet)

# Cargar el archivo .rds
my_data <- readRDS("data/scraping-pagina12-2023-10-09.rds")


my_data %>% 
  filter(stringr::str_detect(content,"narcotráfico")) -> narcotrafico

my_data %>% 
  filter(stringr::str_detect(content,"narco")) -> narco

my_data %>% 
  filter(stringr::str_detect(content,"droga")) -> droga


data <- bind_rows(narcotrafico, narco, droga)

# Accede a la tercera columna "content"
textos <- data$content

# Crear un corpus de texto
corpus <- Corpus(VectorSource(textos))

# Preprocesamiento del texto
corpus <- tm_map(corpus, content_transformer(tolower))  # Convierte a minúsculas
corpus <- tm_map(corpus, removePunctuation)            # Elimina puntuación
corpus <- tm_map(corpus, removeNumbers)               # Elimina números

# Cargar una lista de stopwords en español (o el idioma de tu texto)
stopwords <- stopwords("spanish")

# Eliminar stopwords
corpus <- tm_map(corpus, removeWords, stopwords)


# Crear una matriz de términos (Document-Term Matrix)
dtm <- DocumentTermMatrix(corpus)

# Lista de stopwords personalizadas que deseas eliminar del conteo
stopwords_personalizadas <- c("“", "dos", "años", "dijo", "ser","hace","después","foto")

# Eliminar stopwords personalizadas
dtm <- dtm[, !(colnames(dtm) %in% stopwords_personalizadas)]

# Calcular la suma de frecuencias de cada palabra en todo el corpus
word_freq <- colSums(as.matrix(dtm))

# Ordenar las palabras por frecuencia en orden descendente
word_freq <- sort(word_freq, decreasing = TRUE)

# Mostrar las palabras más frecuentes y sus frecuencias
head(word_freq, 10)


```

 